---
author: '[Matt Lowes](mailto:email@oneacrefund.org)'
date: '`r format(Sys.time(), "%B %d, %Y")`'
output:
  html_notebook:
    number_sections: yes
    code_folding: show
    theme: flatly
    toc: yes
    toc_depth: 6
    toc_float: yes
    css: static/styles.css
---
<title>Title</title>
```{r setup, include=FALSE}
#### set up
## clear environment and console
rm(list = ls())
cat("\014")

## set up some global options
# always set stringsAsFactors = F when loading data
options(stringsAsFactors=FALSE)

# show the code
knitr::opts_chunk$set(echo = TRUE)

# define all knitr tables to be html format
options(knitr.table.format = 'html')

# change code chunk default to not show warnings or messages
knitr::opts_chunk$set(warning = FALSE, message = FALSE)

## load libraries
libs <- c("tidyverse", "knitr", "readxl", "curl", "raster", "rgdal", "velox")
lapply(libs, require, character.only = T, quietly = T, warn.conflicts = F)

#### define helpful functions
# define function to adjust table widths
html_table_width <- function(kable_output, width) {
  width_html <- paste0(paste0('<col width="', width, '">'), collapse = "\n")
  sub("<table>", paste0("<table>\n", width_html), kable_output)
}
options(readr.show_progress = FALSE)
select <- dplyr::select
```

https://hunzikp.github.io/velox/index.html

# Data

Load the data I accessed from the Arc2 weather FTP in the `data_download.R` file. I utilize the `velox` package to speed up the cropping and extraction of data from large sets of rasters. Here's the [velox manual](https://cran.r-project.org/web/packages/velox/velox.pdf) and [velox homepage](http://philipphunziker.com/velox/).

# Functions

There's a limit to how many seasons I can combine. I want to aggregate the data so that it's easy to get the full picture or any relevant subsets What rules should I follow for determining which subsets to make? For now I should just make the long term average >> and set this us to easily update as we get more data.

**HOW TO HANDLE THE LARGE AMOUNT OF DATA** subset the rasters (in groups if need be) to just the areas we're interested in. For the purposes of updating the data going forward, we'll create decade long groupings which should be manageable for extracting values and then summarizing them.

> I'm assuming that the key use cases are feeding in a set of GPS points and getting back an aggregated calculation. 

There are a couple steps I want to follow:

* The `arcWeatherRaw.Rdata` file is large. I don't want to be working with that format. I want to use `velox` to speed up the calculations.
* When I switch to `velox` however, I loose the easier labels of a data.frame object. I need to therefore know what I want to do to the data before putting it into `velox` format I so get back exactly what I want.
* This likely involves creating the set of functions that can subset the `arcWeatherRaw` data:
  + down to the DATES I need (using `subsetYear`, `subsetMonth`, `subsetYearMonth`)
  + storing those labels
  + converting the data to velox to trim it down
  + extract GPS points
    + make sure those GPS points are in the right CRS!
  + aggregate the data as I need
  + convert the labels accordingly and match it back with the aggregated velox data.
  
I'll end up with some nested functions that will, in order:

* Subset the data to the dates of interest
* Convert to `velox`.
* Crop to OAF countries (or is there a way to do this earlier?)
* Extract the GPS points of interest.

## Functions to subset

* Subset by year - subset the raster list by year 
* Subset by month - subset the raster list by month

```{r}

# SUBSET BY YEAR
subsetYear <- function(rList, year){
  # input - list of weather rasters and year as INT.
  # output - year or years in question
  
  # consider adding in some checks that prevent erroneous years from being entered, too big or too small.
  year = as.character(year)
  
  if(length(year) > 1){
    year = paste(year, collapse = "|")
  }
  
  nameVector = lapply(rList, function(x) names(x))
  loc = grep(year, nameVector)
  res = rList[loc]
  return(res)
}

# SUBSET BY MONTH
subsetMonth <- function(rList, month){
  # input - list of weather rasters and month as numeric month
  # output - list of rasters corresopnding to that month
  if(month < 9){
    month = as.character(paste0(0, month))
  } else {
    month = as.character(month)
  }
  
  if(length(month) > 1){
    month = paste(month, collapse = "|")
  }
  
  nameVector = lapply(rList, function(x) names(x))
  
  # grepping the month will be a bit more tricky because we can't simply enter a
  # number as that number will appear many other times for other reasons. So our pattern matching will need to be better!
  grepText = paste0("....", month, "..\\.gz$")
  loc = grep(grepText, nameVector)
  res = rList[loc]
  return(res)
  
}

# I can now use these funcitons in concert to get data from a specific year and month
subsetYearMonth <- function(rList, year, month){
  return(subsetYear(subsetMonth(rList, month), year))
  
}

test <- subsetYearMonth(arcWeatherRaw, 1987, 11)
test <- subsetYear(arcWeatherRaw, 1987)

```

## Functions to convert to `velox`

```{r}
dateDfCreator <- function(dateVector){
  # input: standard format of YYYYMMDD 
  # output: df with three columsn for aggreation
  
  numbers <- as.numeric(gsub(".*?([0-9]+).*", "\\1", dateVector))
  
  year = as.numeric(substr(numbers, 1, 4))
  month = as.numeric(substr(numbers, 5, 6))
  day = as.numeric(substr(numbers, 7, 8))
  
  return(data.frame(numbers, year, month, day))
}


convert_to_velox <- function(rList){
  # input: takes the subset data and convets to velox. Works with list input
  # output: a list of two objects: 
  # a list of velox rasters for additional manipulation
  # a df of the labels to associate with the rasters so that I have something to work with when aggregating the data.
    veloxLoop <- list()
    for(i in seq_along(rList)){
      tmp <- velox(rList[[i]])
      #tmp$crop(oafAreaReproject)
      veloxLoop[[i]] <- tmp
    }
    
    # and create df of the labels from rList
    nameList <- lapply(rList, function(x) names(x))
    nameDf <- data.frame(label = do.call(rbind, nameList))
    metaDf <- dateDfCreator(nameDf$label)
    
    return(list(veloxLoop, metaDf))
}


oafTrim <- function(veloxObj){
  # input: list of velox raster but use the first element of the object
  # output: list of velox raster trimmed to the OAF extent to simplify the calculations to come. 
  # notes: This step might be unncessary? In that case we can probably pretty easily remove it from the sequence.
  
    for(i in seq_along(veloxObj)){
      veloxObj[[i]]$crop(oafAreaReproject)
    }
    return(veloxObj)
}
```

```{r}
checkCRS <- function(veloxRasterList, spdf){
  # inputs: a veloxRasterList and a spdf of GPS points. We'll work with the first element of the VeloxRaster after checking that all are the same CRS.
  # outputs: spdf in the right format if it needs to be transformed
  # notes: This should only be run once for the entire operation and thus it should be done before entering the extraction phase.
  if(class(veloxRasterList[[1]])!="VeloxRaster"){
    stop("\n veloxRaster object needs to be a velox object. First convert to Velox")
  }
  
  if(class(spdf)!="SpatialPointsDataFrame"){
    stop("\n spdf object not a SpatialPointsDataFrame. First convert to SPDF using sp package")
  }
  
  # check if the CRS are already the same. If not convert the SPDF to the velox raster format and report what the format is. This assumes that the veloxRaster all have the CRS which they should because the ARC 2 weather data should. Maybe this is a bad assumption though!
  
  # first check that velox raster list are all the same CRS
  veloxCrs <- lapply(veloxRasterList, function(x){x$crs})
  veloxCrsCheck <- length(unique(veloxCrs)) == 1
  
  if(!veloxCrsCheck){
    stop("\n VeloxRaster object has different CRS in different layers. Check this before proceeding")
  }
  
  
  crsFormatCheck <- veloxRasterList[[1]]$crs == proj4string(spdf)
  if(!crsFormatCheck){
    spdf <- spTransform(spdf, CRS(veloxRasterList[[1]]$crs))
  }
}


## function for extracting points and adding in labels
extract_velox_gps <- function(veloxRaster, spdf){
  # input: v$extract_points(siteReproject) style code. 
  # Takes a list of velox raster and extracts the right data points
  # output: a list with two elements: the first element is a df of the extracted values for the given gps points. 
  #The second list is the df of GPS points to later merge with those GPS points for the final output.
  veloxLoop <- veloxRaster[[1]]
  
  ## here is where we check the CRS and confirm they're compatible.
  checkCRS(veloxLoop, spdf)
  
  ## here is where we extract the values for the given GPS values.
  extractLoop <- list()
  for(i in seq_along(veloxLoop)){
    extractLoop[[i]] <- veloxLoop[[i]]$extract_points(spdf)
    extractLoop[[i]] <- as.data.frame(extractLoop[[i]])
  }
  
  # map the metaDf to the list so that I know the values of each extracted value. Keep this long?
 # extract loop has # of elements equal to the dates and each element is the length of the GPS points. I therefore want to combine dates with each element.
  # convert spdf@data into equal length list for easy mapping
  spdfList <- rep(list(spdf@data), nrow(extractLoop[[1]]))
  
  # combine gps points with extracted values
  dateList <- Map(cbind, spdfList, extractLoop)
  
  #combine date labels with the gps points and extracted values so the data can be aggregated as desired based on dates
  fullDfList <- Map(cbind, split(veloxRaster[[2]], veloxRaster[[2]]$numbers), dateList)
  
  fullDf <- plyr::rbind.fill(fullDfList)
  
  names(fullDf)[names(fullDf) == "V1"] <- "rainfall"
  
  # return the combined meta data and the extracted values
  return(fullDf)
  
}

```

# Testing set up

## Data

```{r}
forceUpdate <- FALSE

dataDir <- normalizePath(file.path("..", "arc2_weather_data"))

rawDir <- normalizePath(file.path("..", "arc2_weather_data", "raw_data"))

# this unfortunately just takes a while to load. 
load(paste(rawDir, "rdata_file", "weatherRasterList.Rdata", sep = "/")) # this is huge! 


mandelaUpdate <- FALSE
if(mandelaUpdate){
  ## toy data for practicing
  toyData <- arcWeatherRaw[1:100]

  saveRDS(toyData, file = paste(rawDir, "toyData.rds", sep = "/"))
}

```

## Kenya shapefile

```{r}
# import the Kenya shapefile, focus on western Kenya and then subset for certain years
gpsDir <- normalizePath(file.path("..", "..", "soil_grids_data"))

siteGpsFiles <- list.files(paste(gpsDir, "2019", sep = "/"), pattern = ".xls")

siteGpsFiles <- siteGpsFiles[!siteGpsFiles %in% c("District Offices.xlsx", "Regional Office.xlsx", "Warehouses.xlsx")]
siteGpsFiles <- paste(gpsDir, "2019", siteGpsFiles, sep = "/")

readGpsFiles <- lapply(siteGpsFiles, function(x){
  read_xlsx(x)
})

combineGpsFiles <- as.data.frame(do.call(rbind, readGpsFiles))

# investigate the projection of these data points
siteSp <- SpatialPointsDataFrame(coords = combineGpsFiles[,c("Longitude", "Latitude")], data = combineGpsFiles, proj4string = CRS("+proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0")) # use this to get the right map for N and P

if(!file.exists(paste0(dataDir, "/GADM_2.8_KEN_adm2.rds"))){
  keBoundaries <- getData("GADM", country='KE', level=2, path = dataDir) 
} else {
  keBoundaries <- readRDS(paste0(dataDir, "/GADM_2.8_KEN_adm2.rds"))
}
```

## Align CRS

I additionally want to subset this down to just western Kenya because it's too computationally intentsive to run these calculations for other areas. I'm going to subset the shape file to just western Kenya and the plot it to confirm I've done it correctly. 

* original crs for site points in Kenya = +proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0 
* original crs for ke boundaries = +proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0
* original crs for weather data = +proj=longlat +a=6371000 +b=6371000 +no_defs

I suggest we use the weather data crs to avoid having lots of intensive reprojection

```{r}
siteReproject <- spTransform(siteSp, CRS("+proj=longlat +a=6371000 +b=6371000 +no_defs"))
boundaryReproject <- spTransform(keBoundaries, CRS("+proj=longlat +a=6371000 +b=6371000 +no_defs"))

weatherMap <- intersect(boundaryReproject, siteReproject)
```

I'm still not really sure what the most expendient way is to get the data we need. I think the best sequence will be extracting the points and then subsetting down by years or months as need be depending on the use case and aggregation

This means I need a full list of gps across all countries in the right CRS. I then can subset down to just the countries I need to make the data smaller. This is hopefully small enough that I can then extract the points I need from a complete brick and then perform the right aggregations by season, year, and month.

```{r}
### FIRST DOWNLOAD THE COUNTRIES OF INTEREST AND COMBINE INTO ONE SP FILE?
# use ISO codes
oafCountries <- c("KEN", "RWA", "UGA", "BDI", "TZA", "ZMB", "MWI") 

# this imports the shape files I need.
countrySpFiles <- lapply(oafCountries, function(x){
  getData("GADM", country = x, level = 2, path = dataDir)
})

gadmFiles <- paste(dataDir, list.files(dataDir, pattern = "GADM"), sep = "/")

# check crs to make sure the files play nicely together
crsCheck <- lapply(gadmFiles, function(x){
  tmp = readRDS(x)
  coordsystem = crs(tmp)@projargs
  return(coordsystem)
})

# then check that the results are the same
length(unique(crsCheck))==1

# combine into one sp file for subsetting the weather data
oafArea <- do.call(bind, lapply(gadmFiles, function(x){readRDS(x)}))

# this is what I now what to use to subset the large and unwieldy weather data, by decade or year if need be. use velox to speed this up!
oafAreaReproject <- spTransform(oafArea, CRS("+proj=longlat +a=6371000 +b=6371000 +no_defs"))
```

```{r}
test <- subsetYear(arcWeatherRaw, 1985)

veloxRaster <- convert_to_velox(test)
spdf = siteSp

test3 <- oafTrim(test2[[1]])
```

# Appendix

**Velox notes:**

Investigate the `oafoperatingarea` object. I want this to have a matrix for each day for each location which means that it'll be a very long list of matrices. I think I really just want to create a big velox stack and extract the points I want taking the mean of that object. That should be manageable.

`test2 <- test$extract(oafAreaReproject, fun=mean)` - this is a 1149 x 365 matrix, there is one column per band in the stack and one row for each object in oafAreaProject

`test3 <- test$extract(oafAreaReproject, fun=NULL)` - this is a list of 1149 objects with 365 columns per element. Those are the values 

