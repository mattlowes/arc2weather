---
author: '[Matt Lowes](mailto:email@oneacrefund.org)'
date: '`r format(Sys.time(), "%B %d, %Y")`'
output:
  html_notebook:
    number_sections: yes
    code_folding: show
    theme: flatly
    toc: yes
    toc_depth: 6
    toc_float: yes
    css: static/styles.css
---
<title>Title</title>
```{r setup, include=FALSE}
#### set up
## clear environment and console
rm(list = ls())
cat("\014")

## set up some global options
# always set stringsAsFactors = F when loading data
options(stringsAsFactors=FALSE)

# show the code
knitr::opts_chunk$set(echo = TRUE)

# define all knitr tables to be html format
options(knitr.table.format = 'html')

# change code chunk default to not show warnings or messages
knitr::opts_chunk$set(warning = FALSE, message = FALSE)

## load libraries
# dplyr and tibble are for working with tables
# reshape is for easy table transformation
# knitr is to make pretty tables at the end
# ggplot2 is for making graphs
# readxl is for reading in Excel files
# MASS is for running boxcox tests
# gridExtra is for arranging plots
# cowplot is for adding subtitles to plots
# robustbase is to run robust regressions to compensate for outliers
# car is for performing logit transformations
libs <- c("tidyverse", "knitr", "readxl", "curl", "raster", "rgdal", "velox")
lapply(libs, require, character.only = T, quietly = T, warn.conflicts = F)

#### define helpful functions
# define function to adjust table widths
html_table_width <- function(kable_output, width) {
  width_html <- paste0(paste0('<col width="', width, '">'), collapse = "\n")
  sub("<table>", paste0("<table>\n", width_html), kable_output)
}
options(readr.show_progress = FALSE)
select <- dplyr::select
```

https://hunzikp.github.io/velox/index.html

# Data

Load the data I accessed from the Arc2 weather FTP in the `data_download.R` file. I utilize the `velox` package to speed up the cropping and extraction of data from large sets of rasters. Here's the [velox manual](https://cran.r-project.org/web/packages/velox/velox.pdf) and [velox homepage](http://philipphunziker.com/velox/).

```{r}
forceUpdate <- FALSE

dataDir <- normalizePath(file.path("..", "arc2_weather_data"))

rawDir <- normalizePath(file.path("..", "arc2_weather_data", "raw_data"))

# this only runs if forceUpate is TRUE
if(!file.exists(paste(rawDir, "rdata_file", "weatherRasterList.Rdata", sep = "/")) & forceUpdate){
  load(paste(rawDir, "rdata_file", "weatherRasterList.Rdata", sep = "/")) # this is huge! Find a way to break this down to simplify calculations. 
  
  ## toy data for practicing
  toyData <- arcWeatherRaw[1:100]

  saveRDS(toyData, file = paste(rawDir, "toyData.rds", sep = "/"))
} 

```


# Kenya shapefile

```{r}
# import the Kenya shapefile, focus on western Kenya and then subset for certain years
gpsDir <- normalizePath(file.path("..", "..", "soil_grids_data"))

siteGpsFiles <- list.files(paste(gpsDir, "2019", sep = "/"), pattern = ".xls")

siteGpsFiles <- siteGpsFiles[!siteGpsFiles %in% c("District Offices.xlsx", "Regional Office.xlsx", "Warehouses.xlsx")]
siteGpsFiles <- paste(gpsDir, "2019", siteGpsFiles, sep = "/")

readGpsFiles <- lapply(siteGpsFiles, function(x){
  read_xlsx(x)
})

combineGpsFiles <- as.data.frame(do.call(rbind, readGpsFiles))

# investigate the projection of these data points
siteSp <- SpatialPointsDataFrame(coords = combineGpsFiles[,c("Longitude", "Latitude")], data = combineGpsFiles, proj4string = CRS("+proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0")) # use this to get the right map for N and P

if(!file.exists(paste0(dataDir, "/GADM_2.8_KEN_adm2.rds"))){
  keBoundaries <- getData("GADM", country='KE', level=2, path = dataDir) 
} else {
  keBoundaries <- readRDS(paste0(dataDir, "/GADM_2.8_KEN_adm2.rds"))
}
```

# Align CRS

I additionally want to subset this down to just western Kenya because it's too computationally intentsive to run these calculations for other areas. I'm going to subset the shape file to just western Kenya and the plot it to confirm I've done it correctly. 

* original crs for site points in Kenya = +proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0 
* original crs for ke boundaries = +proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0
* original crs for weather data = +proj=longlat +a=6371000 +b=6371000 +no_defs

I suggest we use the weather data crs to avoid having lots of intensive reprojection

```{r}
siteReproject <- spTransform(siteSp, CRS("+proj=longlat +a=6371000 +b=6371000 +no_defs"))
boundaryReproject <- spTransform(keBoundaries, CRS("+proj=longlat +a=6371000 +b=6371000 +no_defs"))

weatherMap <- intersect(boundaryReproject, siteReproject)
```

I'm still not really sure what the most expendient way is to get the data we need. I think the best sequence will be extracting the points and then subsetting down by years or months as need be depending on the use case and aggregation

This means I need a full list of gps across all countries in the right CRS. I then can subset down to just the countries I need to make the data smaller. This is hopefully small enough that I can then extract the points I need from a complete brick and then perform the right aggregations by season, year, and month.

```{r}
### FIRST DOWNLOAD THE COUNTRIES OF INTEREST AND COMBINE INTO ONE SP FILE?
# use ISO codes
oafCountries <- c("KEN", "RWA", "UGA", "BDI", "TZA", "ZMB", "MWI") 

# this imports the shape files I need.
countrySpFiles <- lapply(oafCountries, function(x){
  getData("GADM", country = x, level = 2, path = dataDir)
})

gadmFiles <- paste(dataDir, list.files(dataDir, pattern = "GADM"), sep = "/")

# check crs to make sure the files play nicely together
crsCheck <- lapply(gadmFiles, function(x){
  tmp = readRDS(x)
  coordsystem = crs(tmp)@projargs
  return(coordsystem)
})

# then check that the results are the same
length(unique(crsCheck))==1

# combine into one sp file for subsetting the weather data
oafArea <- do.call(bind, lapply(gadmFiles, function(x){readRDS(x)}))

# this is what I now what to use to subset the large and unwieldy weather data, by decade or year if need be. use velox to speed this up!
oafAreaReproject <- spTransform(oafArea, CRS("+proj=longlat +a=6371000 +b=6371000 +no_defs"))
```

# Subset and crop with velox

There's a limit to how many seasons I can combine. I want to aggregate the data so that it's easy to get the full picture or any relevant subsets What rules should I follow for determining which subsets to make? For now I should just make the long term average >> and set this us to easily update as we get more data.

**HOW TO HANDLE THE LARGE AMOUNT OF DATA** subset the rasters (in groups if need be) to just the areas we're interested in. For the purposes of updating the data going forward, we'll create decade long groupings which should be manageable for extracting values and then summarizing them

There are a couple steps I want to follow:

* The `arcWeatherRaw.Rdata` file is large. I don't want to be working with that format. I want to use `velox` to speed up the calculations.
* When I switch to `velox` however, I loose the easier labels of a data.frame object. I need to therefore know what I want to do to the data before putting it into `velox` format I so get back exactly what I want.
* This likely involves creating the set of functions that can subset the `arcWeatherRaw` data:
  + down to the DATES I need
  + storing those lables
  + converting the data to velox to trim it down
  + extract GPS points
    + make sure those GPS points are in the right CRS!
  + aggregate the data as I need
  + convert the labels accordingly and match it back with the aggregated velox data.

```{r}
if(!file.exists(paste(dataDir, "veloxLoop.rds", sep = "/")) | forceUpdate){
  veloxLoop <- list()
  for(i in seq_along(arcWeatherRaw)){
    tmp <- velox(arcWeatherRaw[[i]])
    tmp$crop(oafAreaReproject)
    veloxLoop[[i]] <- tmp
    
  }
  saveRDS(veloxLoop, file=paste(dataDir, "veloxLoop.rds", sep = "/"))
  
} else {
  veloxLoop <- readRDS(paste(dataDir, "veloxLoop.rds", sep = "/"))
}
```

```{r}
## check the size of this file to see if we've made our problem a bit more manageable
checkSize <- function(obj, unit){
  return(format(object.size(obj), units = unit))
}

format(object.size(veloxLoop), units = "Kb") # much smaller!
```

# Extract and aggregate with `velox`



## Functions

* Subset by year - subset the raster list by year 
* Subset by month - subset the raster list by month

```{r}
# SUBSET BY YEAR
subsetYear <- function(rList, year){
  # input - list of weather rasters and year as INT.
  # output - year or years in question
  
  # consider adding in some checks that prevent erroneous years from being entered, too big or too small.
  year = as.character(year)
  
  if(length(year) > 1){
    year = paste(year, collapse = "|")
  }
  
  nameVector = lapply(rList, function(x) names(x))
  loc = grep(year, nameVector)
  res = rList[loc]
  return(res)
}

# SUBSET BY MONTH
subsetMonth <- function(rList, month){
  # input - list of weather rasters and month as numeric month
  # output - list of rasters corresopnding to that month
  month = as.character(paste0(0, month))
  
  if(length(month) > 1){
    month = paste(month, collapse = "|")
  }
  
  nameVector = lapply(rList, function(x) names(x))
  
  # grepping the month will be a bit more tricky because we can't simply enter a
  # number as that number will appear many other times for other reasons. So our pattern matching will need to be better!
  grepText = paste0("....", month, "..\\.gz$")
  loc = grep(grepText, nameVector)
  res = rList[loc]
  return(res)
  
}

# I can now use these funcitons in concert to get data from a specific year and month
subsetYearMonth <- function(rList, year, month){
  return(subsetYear(subsetMonth(rList, month), year))
  
}

#test <- subsetYearMonth(arcWeatherRaw, 1983, 1)


## function for extracting points and adding in labels
extract_label <- function(veloxRaster, spdf, varsToKeep = NULL){
  # input: v$extract_points(siteReproject) style code
  # output: additional labels in the matrices that add labels from the spdf into the data.
  # you'd use this when you don't need the velox capacity anymore as this transforms the data from velox into df.
  if(is.null(varsToKeep)){
    return(cbind(spdf@data, veloxRaster))
  } else {
  return(cbind(spdf@data, veloxRaster) %>%
           dplyr::select(one_of(varsToKeep)))
  }
  
}

extractLoop <- list()
for(i in seq_along(veloxLoop)){
  v <- veloxLoop[[i]]
  extractLoop[[i]] <- v$extract_points(siteReproject)
  extractLoop[[i]] <- extract_label(extractLoop[[i]], siteReproject)
}

```


# Appendix

Investigate the `oafoperatingarea` object. I want this to have a matrix for each day for each location which means that it'll be a very long list of matrices. I think I really just want to create a big velox stack and extract the points I want taking the mean of that object. That should be manageable.

`test2 <- test$extract(oafAreaReproject, fun=mean)` - this is a 1149 x 365 matrix, there is one column per band in the stack and one row for each object in oafAreaProject

`test3 <- test$extract(oafAreaReproject, fun=NULL)` - this is a list of 1149 objects with 365 columns per element. Those are the values 

